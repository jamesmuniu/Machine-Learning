{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The steps you are going to cover in this tutorial are as follows:\n",
    "\n",
    "#Load Data.\n",
    "#Define Model.\n",
    "#Compile Model.\n",
    "#Fit Model.\n",
    "#Evaluate Model.\n",
    "#Tie It All Together.\n",
    "\n",
    "#This tutorial has a few requirements:\n",
    "\n",
    "#You have Python 2 or 3 installed and configured.\n",
    "#You have SciPy (including NumPy) installed and configured.\n",
    "#You have Keras and a backend (Theano or TensorFlow) installed and configured.\n",
    "\n",
    "#1. Load Data\n",
    "\n",
    "#Whenever we work with machine learning algorithms that use a stochastic process (e.g. random numbers), it is a good idea to set the random number seed.\n",
    "\n",
    "#This is so that you can run the same code again and again and get the same result. This is useful if you need to demonstrate a result, compare algorithms using the same source of randomness or to debug a part of your code.\n",
    "\n",
    "#You can initialize the random number generator with any seed you like, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/james/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2885: FutureWarning: \n",
      "mpl_style had been deprecated and will be removed in a future version.\n",
      "Use `matplotlib.pyplot.style.use` instead.\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Dense\n",
    "from keras import optimizers\n",
    "#from keras.layers import Input, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.regularizers import l2 # L2-regularisation\n",
    "from keras.optimizers import SGD\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "pd.options.display.mpl_style = 'default'\n",
    "plt.style.use('ggplot') \n",
    "import numpy as np\n",
    "from sklearn.metrics import log_loss\n",
    "import os\n",
    "#fix random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "# the matplotlib inline command is important, it tells jupyter notebook to show the output of the cell for charts\n",
    "%matplotlib inline\n",
    "\n",
    "poverty_data = os.path.join('poverty_data1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#1. Load Data\n",
    "#In this tutorial, we are going to use the Pima Indians onset of diabetes dataset.\n",
    "#Download it from: \n",
    "#http://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\n",
    "#You can now load the file directly using the NumPy function loadtxt().\n",
    "#There are eight input variables and one output variable (the last column).\n",
    "#Once loaded we can split the dataset into input variables (X) and the output class variable (Y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load pima indians dataset\n",
    "data_paths = {'A': {'train': os.path.join(poverty_data, 'A', 'A_hhold_train.csv'),\n",
    "                    'train_indiv':  os.path.join(poverty_data, 'A', 'A_indiv_train.csv'),\n",
    "                    'test':  os.path.join(poverty_data, 'A', 'A_hhold_test.csv'),\n",
    "                    'test_indiv':  os.path.join(poverty_data, 'A', 'A_indiv_test.csv')}, \n",
    "              \n",
    "              'B': {'train': os.path.join(poverty_data, 'B', 'B_hhold_train.csv'),\n",
    "                    'train_indiv': os.path.join(poverty_data, 'B', 'B_indiv_train.csv'),\n",
    "                    'test_indiv': os.path.join(poverty_data, 'B', 'B_indiv_test.csv'),\n",
    "                    'test':  os.path.join(poverty_data, 'B', 'B_hhold_test.csv')}, \n",
    "              \n",
    "              'C': {'train': os.path.join(poverty_data, 'C', 'C_hhold_train.csv'), \n",
    "                    'train_indiv': os.path.join(poverty_data, 'C', 'C_indiv_train.csv'),\n",
    "                    'test_indiv': os.path.join(poverty_data, 'C', 'C_indiv_test.csv'),\n",
    "                    'test':  os.path.join(poverty_data, 'C', 'C_hhold_test.csv')}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load training data\n",
    "a_train = pd.read_csv(data_paths['A']['train'])#, index_col= 'id')\n",
    "\n",
    "b_train = pd.read_csv(data_paths['B']['train'])#, index_col='id')\n",
    "\n",
    "c_train = pd.read_csv(data_paths['C']['train'])#, index_col='id')\n",
    "\n",
    "a_test = pd.read_csv(data_paths['A']['test'], index_col='id')\n",
    "b_test = pd.read_csv(data_paths['B']['test'], index_col='id')\n",
    "c_test = pd.read_csv(data_paths['C']['test'], index_col='id')\n",
    "\n",
    "a_train_indiv = pd.read_csv(data_paths['A']['train_indiv'])#, index_col='id')\n",
    "\n",
    "b_train_indiv = pd.read_csv(data_paths['B']['train_indiv'])#, index_col='id')\n",
    "\n",
    "c_train_indiv = pd.read_csv(data_paths['C']['train_indiv'])#, index_col='id')\n",
    "\n",
    "a_test_indiv = pd.read_csv(data_paths['A']['test_indiv'])#, index_col='id')\n",
    "b_test_indiv = pd.read_csv(data_paths['B']['test_indiv'])#, index_col='id')\n",
    "c_test_indiv = pd.read_csv(data_paths['C']['test_indiv'])#, index_col='id')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fea7fa58f90>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/anaconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:1288: UserWarning: findfont: Font family [u'monospace'] not found. Falling back to Bitstream Vera Sans\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEdCAYAAAAPT9w1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEZFE/H9jSo2hYhYkSwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feac85e8490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a_train.poor.value_counts().plot.bar(title='Number of Poor for country A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fea7d21bed0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEdCAYAAAAPT9w1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feac85e8450>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "b_train.poor.value_counts().plot.bar(title='Number of Poor for country B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fea7b9608d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEdCAYAAAAPT9w1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fea7b974050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "c_train.poor.value_counts().plot.bar(title='Number of Poor for country C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    4500\n",
       "True     3703\n",
       "Name: poor, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_train [\"poor\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8203, 346)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    3004\n",
       "True      251\n",
       "Name: poor, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_train [\"poor\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3255, 443)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    5496\n",
       "True      973\n",
       "Name: poor, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_train [\"poor\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6469, 165)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8203 entries, 0 to 8202\n",
      "Columns: 346 entries, id to poor\n",
      "dtypes: bool(1), int64(5), object(340)\n",
      "memory usage: 21.6+ MB\n"
     ]
    }
   ],
   "source": [
    "a_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3255 entries, 0 to 3254\n",
      "Columns: 443 entries, id to poor\n",
      "dtypes: bool(1), float64(9), int64(15), object(418)\n",
      "memory usage: 11.0+ MB\n"
     ]
    }
   ],
   "source": [
    "b_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6469 entries, 0 to 6468\n",
      "Columns: 165 entries, id to poor\n",
      "dtypes: bool(1), float64(1), int64(30), object(133)\n",
      "memory usage: 8.1+ MB\n"
     ]
    }
   ],
   "source": [
    "c_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "def standardize(df, numeric_only=True):\n",
    "    numeric = df.select_dtypes(include=['int64', 'float64'])\n",
    "    \n",
    "    # subtracy mean and divide by std\n",
    "    df[numeric.columns] = (numeric - numeric.mean()) / numeric.std()\n",
    "    #preprocessing.MinMaxScaler()\n",
    "    \n",
    "    return df\n",
    "    \n",
    "\n",
    "def pre_process_data(df, enforce_cols=None):\n",
    "    print(\"Input shape:\\t{}\".format(df.shape))\n",
    "        \n",
    "\n",
    "    df = standardize(df)\n",
    "    print(\"After standardization {}\".format(df.shape))\n",
    "        \n",
    "    # create dummy variables for categoricals\n",
    "    df = pd.get_dummies(df)\n",
    "    print(\"After converting categoricals:\\t{}\".format(df.shape))\n",
    "    \n",
    "\n",
    "    # match test set and training set columns\n",
    "    if enforce_cols is not None:\n",
    "        to_drop = np.setdiff1d(df.columns, enforce_cols)\n",
    "        to_add = np.setdiff1d(enforce_cols, df.columns)\n",
    "\n",
    "        df.drop(to_drop, axis=1, inplace=True)\n",
    "        df = df.assign(**{c: 0 for c in to_add})\n",
    "    \n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Country A\n",
      "Input shape:\t(8203, 346)\n",
      "After standardization (8203, 346)\n",
      "After converting categoricals:\t(8203, 861)\n",
      "Input shape:\t(37560, 44)\n",
      "After standardization (37560, 44)\n",
      "After converting categoricals:\t(37560, 277)\n",
      "\\Country A-TEST\n",
      "Input shape:\t(4041, 344)\n",
      "After standardization (4041, 344)\n",
      "After converting categoricals:\t(4041, 851)\n",
      "Country B-TEST\n",
      "Input shape:\t(1604, 441)\n",
      "After standardization (1604, 441)\n",
      "After converting categoricals:\t(1604, 1419)\n",
      "Country C-TEST\n",
      "Input shape:\t(3187, 163)\n",
      "After standardization (3187, 163)\n",
      "After converting categoricals:\t(3187, 773)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCountry A\")\n",
    "\n",
    "aX_train = pre_process_data(a_train)#.drop)'poor', axis=1))\n",
    "aX_train_indiv = pre_process_data(a_train_indiv)#.drop('poor', axis=1))\n",
    "aX_train = pd.merge(aX_train,aX_train_indiv, on=\"id\",how=\"right\")\n",
    "aX_train.drop('poor_y', axis=1)\n",
    "aX_train.drop('poor_x', axis=1)\n",
    "ay_train = np.ravel(aX_train.poor_y)\n",
    "#ay_train_indiv = np.ravel(a_train_indiv.poor)\n",
    "#pd.merge(aX_train, a_train_indiv, on= 'id', how='outer')\n",
    "#aX_train.merge(a_train_indiv, left_on= index_col, right_on='iid', how='outer')\n",
    "#aX_train = pd.merge(aX_train_indiv, aX_train,on='id',how=\"left\")\n",
    "#aX_train = pre_process_data(a_train.drop('poor', axis=1))\n",
    "#aX_train = pre_process_data(a_train_indiv.drop('poor', axis=1))\n",
    "#aj_train = pd.concat([a_train['poor'],a_train_indiv['poor']],axis=0)\n",
    "#ay_train = pre_process_data(aj_train)\n",
    "\n",
    "#ay_train = np.ravel(a_train.poor)\n",
    "#aX_train_indiv = pre_process_data(a_train_indiv)\n",
    "#aX_train_indiv = pre_process_data(a_train_indiv.drop('iid', axis=1))\n",
    "\n",
    "\n",
    "print(\"\\Country A-TEST\")\n",
    "aX_test = pre_process_data(a_test)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Country B-TEST\")\n",
    "bX_test = pre_process_data(b_test)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Country C-TEST\")\n",
    "cX_test = pre_process_data(c_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#submission = (aX_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#submission.to_csv('aXtrain.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>nEsgxvAq</th>\n",
       "      <th>OMtioXZZ</th>\n",
       "      <th>YFMZwKrU</th>\n",
       "      <th>TiwRslOh</th>\n",
       "      <th>poor_x</th>\n",
       "      <th>wBXbHZmp_DkQlr</th>\n",
       "      <th>wBXbHZmp_JhtDR</th>\n",
       "      <th>SlDKnCuu_GUusz</th>\n",
       "      <th>SlDKnCuu_alLXR</th>\n",
       "      <th>...</th>\n",
       "      <th>rQWIpTiG_xUYIC</th>\n",
       "      <th>XizJGmbu_FUUXv</th>\n",
       "      <th>XizJGmbu_GtHel</th>\n",
       "      <th>XizJGmbu_juMSt</th>\n",
       "      <th>xqUooaNJ_ALcKg</th>\n",
       "      <th>xqUooaNJ_JTCKs</th>\n",
       "      <th>xqUooaNJ_UaIsy</th>\n",
       "      <th>xqUooaNJ_dSJoN</th>\n",
       "      <th>xqUooaNJ_vhhVz</th>\n",
       "      <th>country_A_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.057372</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.057372</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.057372</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.057372</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.344249</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  1137 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  nEsgxvAq  OMtioXZZ  YFMZwKrU  TiwRslOh poor_x  wBXbHZmp_DkQlr  \\\n",
       "0  1.057372       NaN       NaN       NaN       NaN    NaN             NaN   \n",
       "1  1.057372       NaN       NaN       NaN       NaN    NaN             NaN   \n",
       "2  1.057372       NaN       NaN       NaN       NaN    NaN             NaN   \n",
       "3  1.057372       NaN       NaN       NaN       NaN    NaN             NaN   \n",
       "4 -0.344249       NaN       NaN       NaN       NaN    NaN             NaN   \n",
       "\n",
       "   wBXbHZmp_JhtDR  SlDKnCuu_GUusz  SlDKnCuu_alLXR     ...       \\\n",
       "0             NaN             NaN             NaN     ...        \n",
       "1             NaN             NaN             NaN     ...        \n",
       "2             NaN             NaN             NaN     ...        \n",
       "3             NaN             NaN             NaN     ...        \n",
       "4             NaN             NaN             NaN     ...        \n",
       "\n",
       "   rQWIpTiG_xUYIC  XizJGmbu_FUUXv  XizJGmbu_GtHel  XizJGmbu_juMSt  \\\n",
       "0             1.0             0.0             0.0             1.0   \n",
       "1             1.0             0.0             0.0             1.0   \n",
       "2             0.0             0.0             0.0             1.0   \n",
       "3             0.0             0.0             1.0             0.0   \n",
       "4             1.0             0.0             0.0             1.0   \n",
       "\n",
       "   xqUooaNJ_ALcKg  xqUooaNJ_JTCKs  xqUooaNJ_UaIsy  xqUooaNJ_dSJoN  \\\n",
       "0             0.0             0.0             0.0             1.0   \n",
       "1             0.0             1.0             0.0             0.0   \n",
       "2             0.0             1.0             0.0             0.0   \n",
       "3             0.0             1.0             0.0             0.0   \n",
       "4             0.0             0.0             1.0             0.0   \n",
       "\n",
       "   xqUooaNJ_vhhVz  country_A_y  \n",
       "0             0.0          1.0  \n",
       "1             0.0          1.0  \n",
       "2             0.0          1.0  \n",
       "3             0.0          1.0  \n",
       "4             0.0          1.0  \n",
       "\n",
       "[5 rows x 1137 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aX_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37560,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ay_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aX_train = aX_train.dropna(axis=1, how='all')\n",
    "aX_test = aX_test.dropna(axis=1, how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#aX_train = aX_train.fillna(aX_train.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aX_train = aX_train.fillna(0)\n",
    "aX_test = aX_test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aX_train = aX_train.drop('poor_y',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aX_train = aX_train.drop('iid', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aX_train = aX_train.drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37560, 274)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aX_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OdXpbPGJ</th>\n",
       "      <th>ukWqmeSS</th>\n",
       "      <th>HeUgMnzF_BNCcM</th>\n",
       "      <th>HeUgMnzF_HUpWg</th>\n",
       "      <th>HeUgMnzF_JMXQx</th>\n",
       "      <th>HeUgMnzF_PAVsH</th>\n",
       "      <th>HeUgMnzF_SJPkb</th>\n",
       "      <th>HeUgMnzF_SlRmt</th>\n",
       "      <th>HeUgMnzF_TRFeI</th>\n",
       "      <th>HeUgMnzF_XJgvq</th>\n",
       "      <th>...</th>\n",
       "      <th>rQWIpTiG_xUYIC</th>\n",
       "      <th>XizJGmbu_FUUXv</th>\n",
       "      <th>XizJGmbu_GtHel</th>\n",
       "      <th>XizJGmbu_juMSt</th>\n",
       "      <th>xqUooaNJ_ALcKg</th>\n",
       "      <th>xqUooaNJ_JTCKs</th>\n",
       "      <th>xqUooaNJ_UaIsy</th>\n",
       "      <th>xqUooaNJ_dSJoN</th>\n",
       "      <th>xqUooaNJ_vhhVz</th>\n",
       "      <th>country_A_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.223762</td>\n",
       "      <td>0.805895</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.223762</td>\n",
       "      <td>0.370142</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.223762</td>\n",
       "      <td>-0.719240</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.991586</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.223762</td>\n",
       "      <td>2.984660</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  274 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   OdXpbPGJ  ukWqmeSS  HeUgMnzF_BNCcM  HeUgMnzF_HUpWg  HeUgMnzF_JMXQx  \\\n",
       "0 -0.223762  0.805895             0.0             0.0             0.0   \n",
       "1 -0.223762  0.370142             0.0             0.0             0.0   \n",
       "2 -0.223762 -0.719240             0.0             0.0             0.0   \n",
       "3  0.000000 -0.991586             0.0             0.0             0.0   \n",
       "4 -0.223762  2.984660             0.0             0.0             0.0   \n",
       "\n",
       "   HeUgMnzF_PAVsH  HeUgMnzF_SJPkb  HeUgMnzF_SlRmt  HeUgMnzF_TRFeI  \\\n",
       "0             0.0             0.0             0.0             0.0   \n",
       "1             0.0             0.0             0.0             0.0   \n",
       "2             0.0             0.0             0.0             1.0   \n",
       "3             0.0             0.0             0.0             0.0   \n",
       "4             0.0             0.0             0.0             0.0   \n",
       "\n",
       "   HeUgMnzF_XJgvq     ...       rQWIpTiG_xUYIC  XizJGmbu_FUUXv  \\\n",
       "0             0.0     ...                  1.0             0.0   \n",
       "1             0.0     ...                  1.0             0.0   \n",
       "2             0.0     ...                  0.0             0.0   \n",
       "3             0.0     ...                  0.0             0.0   \n",
       "4             0.0     ...                  1.0             0.0   \n",
       "\n",
       "   XizJGmbu_GtHel  XizJGmbu_juMSt  xqUooaNJ_ALcKg  xqUooaNJ_JTCKs  \\\n",
       "0             0.0             1.0             0.0             0.0   \n",
       "1             0.0             1.0             0.0             1.0   \n",
       "2             0.0             1.0             0.0             1.0   \n",
       "3             1.0             0.0             0.0             1.0   \n",
       "4             0.0             1.0             0.0             0.0   \n",
       "\n",
       "   xqUooaNJ_UaIsy  xqUooaNJ_dSJoN  xqUooaNJ_vhhVz  country_A_y  \n",
       "0             0.0             1.0             0.0          1.0  \n",
       "1             0.0             0.0             0.0          1.0  \n",
       "2             0.0             0.0             0.0          1.0  \n",
       "3             0.0             0.0             0.0          1.0  \n",
       "4             1.0             0.0             0.0          1.0  \n",
       "\n",
       "[5 rows x 274 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aX_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4041, 851)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aX_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, ..., False, False, False])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ay_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Country B\n",
      "Input shape:\t(3255, 443)\n",
      "After standardization (3255, 443)\n",
      "After converting categoricals:\t(3255, 1434)\n",
      "Input shape:\t(20252, 227)\n",
      "After standardization (20252, 227)\n",
      "After converting categoricals:\t(20252, 1644)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCountry B\")\n",
    "bX_train = pre_process_data(b_train)#.drop)'poor', axis=1))\n",
    "bX_train_indiv = pre_process_data(b_train_indiv)#.drop('poor', axis=1))\n",
    "bX_train = pd.merge(bX_train,bX_train_indiv, on=\"id\",how=\"right\")\n",
    "bX_train.drop('poor_y', axis=1)\n",
    "bX_train.drop('poor_x', axis=1)\n",
    "by_train = np.ravel(bX_train.poor_y)\n",
    "#bX_train = pre_process_data(b_train.drop('poor', axis=1))\n",
    "#by_train = np.ravel(b_train.poor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bX_train = bX_train.drop('poor_y',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bX_train = bX_train.drop('iid', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bX_train = bX_train.drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bX_train = bX_train.dropna(axis=1, how='all')\n",
    "bX_test = bX_test.dropna(axis=1, how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bX_train = bX_train.fillna(0)\n",
    "bX_test = bX_test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#bX_train = bX_train.fillna(bX_train.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, ..., False, False, False])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "by_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BoxViLPz</th>\n",
       "      <th>qlLzyqpP</th>\n",
       "      <th>unRAgFtX</th>\n",
       "      <th>TJGiunYp</th>\n",
       "      <th>WmKLEUcd</th>\n",
       "      <th>DYgxQeEi</th>\n",
       "      <th>jfsTwowc</th>\n",
       "      <th>MGfpfHam</th>\n",
       "      <th>esHWAAyG</th>\n",
       "      <th>DtcKwIEv</th>\n",
       "      <th>...</th>\n",
       "      <th>PxgyaWYq_uojQd</th>\n",
       "      <th>PxgyaWYq_vaEub</th>\n",
       "      <th>PxgyaWYq_xUNcn</th>\n",
       "      <th>PxgyaWYq_ypdjU</th>\n",
       "      <th>PxgyaWYq_zefPK</th>\n",
       "      <th>PxgyaWYq_zvCJK</th>\n",
       "      <th>cavdrXpj_GapoC</th>\n",
       "      <th>cavdrXpj_uJXdA</th>\n",
       "      <th>cavdrXpj_yspXz</th>\n",
       "      <th>country_B_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.222467</td>\n",
       "      <td>0.804137</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.804137</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.835997</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.349666</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.804137</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.056358</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.966927</td>\n",
       "      <td>0.615175</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.056358</td>\n",
       "      <td>0.461272</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  1641 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   BoxViLPz  qlLzyqpP  unRAgFtX  TJGiunYp  WmKLEUcd  DYgxQeEi  jfsTwowc  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000       0.0       0.0       0.0   \n",
       "1  0.000000  0.000000  0.000000  0.000000       0.0       0.0       0.0   \n",
       "2 -1.835997  0.000000  0.000000 -0.349666       0.0       0.0       0.0   \n",
       "3  0.000000  0.000000  0.000000  0.000000       0.0       0.0       0.0   \n",
       "4  0.000000  0.966927  0.615175  0.000000       0.0       0.0       0.0   \n",
       "\n",
       "   MGfpfHam  esHWAAyG  DtcKwIEv     ...       PxgyaWYq_uojQd  PxgyaWYq_vaEub  \\\n",
       "0 -0.222467  0.804137  0.000000     ...                  0.0             0.0   \n",
       "1  0.000000  0.804137  0.000000     ...                  0.0             0.0   \n",
       "2  0.000000  0.804137  0.000000     ...                  0.0             0.0   \n",
       "3  0.000000  0.056358  0.000000     ...                  0.0             0.0   \n",
       "4  0.000000  0.056358  0.461272     ...                  0.0             0.0   \n",
       "\n",
       "   PxgyaWYq_xUNcn  PxgyaWYq_ypdjU  PxgyaWYq_zefPK  PxgyaWYq_zvCJK  \\\n",
       "0             0.0             0.0             0.0             0.0   \n",
       "1             0.0             0.0             0.0             0.0   \n",
       "2             0.0             0.0             0.0             0.0   \n",
       "3             0.0             0.0             0.0             0.0   \n",
       "4             0.0             0.0             0.0             0.0   \n",
       "\n",
       "   cavdrXpj_GapoC  cavdrXpj_uJXdA  cavdrXpj_yspXz  country_B_y  \n",
       "0             0.0             1.0             0.0          1.0  \n",
       "1             0.0             1.0             0.0          1.0  \n",
       "2             0.0             1.0             0.0          1.0  \n",
       "3             0.0             1.0             0.0          1.0  \n",
       "4             0.0             1.0             0.0          1.0  \n",
       "\n",
       "[5 rows x 1641 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bX_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Country C\n",
      "Input shape:\t(6469, 165)\n",
      "After standardization (6469, 165)\n",
      "After converting categoricals:\t(6469, 797)\n",
      "Input shape:\t(29913, 44)\n",
      "After standardization (29913, 44)\n",
      "After converting categoricals:\t(29913, 303)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCountry C\")\n",
    "cX_train = pre_process_data(c_train)#.drop)'poor', axis=1))\n",
    "cX_train_indiv = pre_process_data(c_train_indiv)#.drop('poor', axis=1))\n",
    "cX_train = pd.merge(cX_train,cX_train_indiv,on=\"id\",how=\"right\")\n",
    "cX_train.drop('poor_y', axis=1)\n",
    "cX_train.drop('poor_x', axis=1)\n",
    "cy_train = np.ravel(cX_train.poor_y)\n",
    "#cX_train = pre_process_data(c_train.drop('poor', axis=1))\n",
    "#cy_train = np.ravel(c_train.poor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cX_train = cX_train.drop('poor_y', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cX_train = cX_train.drop('iid', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cX_train = cX_train.drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cX_train = cX_train.dropna(axis=1, how='all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cX_train = cX_train.fillna(cX_train.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cX_train = cX_train.dropna(axis=1, how='all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>XKQWlRjk</th>\n",
       "      <th>vWNISgEA</th>\n",
       "      <th>bsMfXBld</th>\n",
       "      <th>XKyOwsRR</th>\n",
       "      <th>CgAkQtOd</th>\n",
       "      <th>OoqEwyJF_RuCZA</th>\n",
       "      <th>OoqEwyJF_cEcbt</th>\n",
       "      <th>cJPCnaAs_BZKME</th>\n",
       "      <th>cJPCnaAs_DMxNA</th>\n",
       "      <th>cJPCnaAs_Eadzw</th>\n",
       "      <th>...</th>\n",
       "      <th>sCTSWhXf_yQhuJ</th>\n",
       "      <th>rVneGwzn_QGHnL</th>\n",
       "      <th>rVneGwzn_UXHpZ</th>\n",
       "      <th>rVneGwzn_ldKFc</th>\n",
       "      <th>rVneGwzn_xgpHA</th>\n",
       "      <th>uVFOfrpa_DnIbO</th>\n",
       "      <th>uVFOfrpa_kXobL</th>\n",
       "      <th>uVFOfrpa_oacjJ</th>\n",
       "      <th>uVFOfrpa_xRxWC</th>\n",
       "      <th>country_C_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.459952</td>\n",
       "      <td>-0.420516</td>\n",
       "      <td>0.402120</td>\n",
       "      <td>-0.710936</td>\n",
       "      <td>0.390934</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.357733</td>\n",
       "      <td>-0.420516</td>\n",
       "      <td>0.402120</td>\n",
       "      <td>-0.710936</td>\n",
       "      <td>0.390934</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.009835</td>\n",
       "      <td>-0.420516</td>\n",
       "      <td>0.402120</td>\n",
       "      <td>-0.710936</td>\n",
       "      <td>0.390934</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.009835</td>\n",
       "      <td>2.584349</td>\n",
       "      <td>-2.676993</td>\n",
       "      <td>0.186228</td>\n",
       "      <td>-1.914076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.009835</td>\n",
       "      <td>2.869020</td>\n",
       "      <td>-3.528663</td>\n",
       "      <td>0.143506</td>\n",
       "      <td>-2.064371</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   XKQWlRjk  vWNISgEA  bsMfXBld  XKyOwsRR  CgAkQtOd  OoqEwyJF_RuCZA  \\\n",
       "0  2.459952 -0.420516  0.402120 -0.710936  0.390934             0.0   \n",
       "1  1.357733 -0.420516  0.402120 -0.710936  0.390934             0.0   \n",
       "2 -0.009835 -0.420516  0.402120 -0.710936  0.390934             0.0   \n",
       "3 -0.009835  2.584349 -2.676993  0.186228 -1.914076             0.0   \n",
       "4 -0.009835  2.869020 -3.528663  0.143506 -2.064371             0.0   \n",
       "\n",
       "   OoqEwyJF_cEcbt  cJPCnaAs_BZKME  cJPCnaAs_DMxNA  cJPCnaAs_Eadzw  \\\n",
       "0             1.0             0.0             0.0             0.0   \n",
       "1             1.0             0.0             0.0             0.0   \n",
       "2             1.0             0.0             0.0             0.0   \n",
       "3             1.0             0.0             0.0             0.0   \n",
       "4             1.0             0.0             0.0             0.0   \n",
       "\n",
       "      ...       sCTSWhXf_yQhuJ  rVneGwzn_QGHnL  rVneGwzn_UXHpZ  \\\n",
       "0     ...                  1.0             0.0             0.0   \n",
       "1     ...                  1.0             0.0             0.0   \n",
       "2     ...                  1.0             0.0             0.0   \n",
       "3     ...                  1.0             1.0             0.0   \n",
       "4     ...                  1.0             1.0             0.0   \n",
       "\n",
       "   rVneGwzn_ldKFc  rVneGwzn_xgpHA  uVFOfrpa_DnIbO  uVFOfrpa_kXobL  \\\n",
       "0             0.0             1.0             1.0             0.0   \n",
       "1             0.0             1.0             1.0             0.0   \n",
       "2             1.0             0.0             0.0             1.0   \n",
       "3             0.0             0.0             0.0             0.0   \n",
       "4             0.0             0.0             0.0             0.0   \n",
       "\n",
       "   uVFOfrpa_oacjJ  uVFOfrpa_xRxWC  country_C_y  \n",
       "0             0.0             0.0          1.0  \n",
       "1             0.0             0.0          1.0  \n",
       "2             0.0             0.0          1.0  \n",
       "3             0.0             1.0          1.0  \n",
       "4             0.0             1.0          1.0  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cX_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, ..., False, False, False])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cy_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a_num_features = aX_train.shape[1]\n",
    "b_num_features = bX_train.shape[1]\n",
    "c_num_features = cX_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#l1_lambda = 0.01 # use 0.001 as a L2-regularisation factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(200, kernel_initializer = 'uniform', input_dim = a_num_features, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(100, kernel_initializer = 'uniform',  activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, kernel_initializer = 'uniform', activation='sigmoid'))\n",
    "adam = optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "37560/37560 [==============================] - 13s 353us/step - loss: 0.5565 - acc: 0.7088\n",
      "Epoch 2/100\n",
      "37560/37560 [==============================] - 13s 354us/step - loss: 0.5557 - acc: 0.7080\n",
      "Epoch 3/100\n",
      "37560/37560 [==============================] - 14s 360us/step - loss: 0.5532 - acc: 0.7084\n",
      "Epoch 4/100\n",
      "37560/37560 [==============================] - 13s 359us/step - loss: 0.5515 - acc: 0.7101\n",
      "Epoch 5/100\n",
      "37560/37560 [==============================] - 14s 364us/step - loss: 0.5489 - acc: 0.7128\n",
      "Epoch 6/100\n",
      "37560/37560 [==============================] - 14s 364us/step - loss: 0.5479 - acc: 0.7136\n",
      "Epoch 7/100\n",
      "37560/37560 [==============================] - 14s 364us/step - loss: 0.5465 - acc: 0.7135\n",
      "Epoch 8/100\n",
      "37560/37560 [==============================] - 14s 365us/step - loss: 0.5448 - acc: 0.7163\n",
      "Epoch 9/100\n",
      "37560/37560 [==============================] - 14s 364us/step - loss: 0.5425 - acc: 0.7163\n",
      "Epoch 10/100\n",
      "37560/37560 [==============================] - 14s 364us/step - loss: 0.5415 - acc: 0.7183\n",
      "Epoch 11/100\n",
      "37560/37560 [==============================] - 14s 366us/step - loss: 0.5386 - acc: 0.7168\n",
      "Epoch 12/100\n",
      "37560/37560 [==============================] - 14s 368us/step - loss: 0.5389 - acc: 0.7175\n",
      "Epoch 13/100\n",
      "37560/37560 [==============================] - 14s 369us/step - loss: 0.5373 - acc: 0.7178\n",
      "Epoch 14/100\n",
      "37560/37560 [==============================] - 14s 381us/step - loss: 0.5345 - acc: 0.7220\n",
      "Epoch 15/100\n",
      "37560/37560 [==============================] - 14s 377us/step - loss: 0.5331 - acc: 0.7214\n",
      "Epoch 16/100\n",
      "37560/37560 [==============================] - 15s 404us/step - loss: 0.5306 - acc: 0.7256\n",
      "Epoch 17/100\n",
      "37560/37560 [==============================] - 16s 426us/step - loss: 0.5311 - acc: 0.7246\n",
      "Epoch 18/100\n",
      "37560/37560 [==============================] - 17s 445us/step - loss: 0.5297 - acc: 0.7243\n",
      "Epoch 19/100\n",
      "37560/37560 [==============================] - 16s 434us/step - loss: 0.5283 - acc: 0.7258\n",
      "Epoch 20/100\n",
      "37560/37560 [==============================] - 15s 397us/step - loss: 0.5249 - acc: 0.7295\n",
      "Epoch 21/100\n",
      "37560/37560 [==============================] - 15s 395us/step - loss: 0.5244 - acc: 0.7288\n",
      "Epoch 22/100\n",
      "37560/37560 [==============================] - 15s 398us/step - loss: 0.5233 - acc: 0.7274\n",
      "Epoch 23/100\n",
      "37560/37560 [==============================] - 17s 456us/step - loss: 0.5233 - acc: 0.7267\n",
      "Epoch 24/100\n",
      "37560/37560 [==============================] - 18s 468us/step - loss: 0.5231 - acc: 0.7279\n",
      "Epoch 25/100\n",
      "37560/37560 [==============================] - 18s 469us/step - loss: 0.5222 - acc: 0.7295\n",
      "Epoch 26/100\n",
      "37560/37560 [==============================] - 14s 378us/step - loss: 0.5197 - acc: 0.7315\n",
      "Epoch 27/100\n",
      "37560/37560 [==============================] - 15s 411us/step - loss: 0.5172 - acc: 0.7325\n",
      "Epoch 28/100\n",
      "37560/37560 [==============================] - 16s 418us/step - loss: 0.5192 - acc: 0.7294\n",
      "Epoch 29/100\n",
      "37560/37560 [==============================] - 16s 413us/step - loss: 0.5172 - acc: 0.7313\n",
      "Epoch 30/100\n",
      "37560/37560 [==============================] - 15s 412us/step - loss: 0.5176 - acc: 0.7320\n",
      "Epoch 31/100\n",
      "37560/37560 [==============================] - 16s 422us/step - loss: 0.5168 - acc: 0.7343\n",
      "Epoch 32/100\n",
      "37560/37560 [==============================] - 18s 484us/step - loss: 0.5142 - acc: 0.7353\n",
      "Epoch 33/100\n",
      "37560/37560 [==============================] - 19s 506us/step - loss: 0.5142 - acc: 0.7352\n",
      "Epoch 34/100\n",
      "37560/37560 [==============================] - 19s 502us/step - loss: 0.5120 - acc: 0.7357\n",
      "Epoch 35/100\n",
      "37560/37560 [==============================] - 19s 514us/step - loss: 0.5108 - acc: 0.7362\n",
      "Epoch 36/100\n",
      "37560/37560 [==============================] - 20s 522us/step - loss: 0.5106 - acc: 0.7357\n",
      "Epoch 37/100\n",
      "37560/37560 [==============================] - 19s 513us/step - loss: 0.5097 - acc: 0.7369\n",
      "Epoch 38/100\n",
      "37560/37560 [==============================] - 17s 446us/step - loss: 0.5095 - acc: 0.7380\n",
      "Epoch 39/100\n",
      "37560/37560 [==============================] - 17s 445us/step - loss: 0.5080 - acc: 0.7368\n",
      "Epoch 40/100\n",
      "37560/37560 [==============================] - 17s 447us/step - loss: 0.5058 - acc: 0.7362\n",
      "Epoch 41/100\n",
      "37560/37560 [==============================] - 20s 525us/step - loss: 0.5086 - acc: 0.7392\n",
      "Epoch 42/100\n",
      "37560/37560 [==============================] - 21s 546us/step - loss: 0.5055 - acc: 0.7394\n",
      "Epoch 43/100\n",
      "37560/37560 [==============================] - 20s 545us/step - loss: 0.5045 - acc: 0.7402\n",
      "Epoch 44/100\n",
      "37560/37560 [==============================] - 21s 557us/step - loss: 0.5044 - acc: 0.7383\n",
      "Epoch 45/100\n",
      "37560/37560 [==============================] - 21s 562us/step - loss: 0.5038 - acc: 0.7401\n",
      "Epoch 46/100\n",
      "37560/37560 [==============================] - 21s 565us/step - loss: 0.5030 - acc: 0.7414\n",
      "Epoch 47/100\n",
      "37560/37560 [==============================] - 21s 568us/step - loss: 0.5013 - acc: 0.7417\n",
      "Epoch 48/100\n",
      "37560/37560 [==============================] - 21s 569us/step - loss: 0.5009 - acc: 0.7422\n",
      "Epoch 49/100\n",
      "37560/37560 [==============================] - 18s 490us/step - loss: 0.5013 - acc: 0.7414\n",
      "Epoch 50/100\n",
      "37560/37560 [==============================] - 18s 474us/step - loss: 0.5004 - acc: 0.7423\n",
      "Epoch 51/100\n",
      "37560/37560 [==============================] - 18s 478us/step - loss: 0.4995 - acc: 0.7432\n",
      "Epoch 52/100\n",
      "37560/37560 [==============================] - 18s 483us/step - loss: 0.5001 - acc: 0.7420\n",
      "Epoch 53/100\n",
      "37560/37560 [==============================] - 18s 481us/step - loss: 0.4982 - acc: 0.7441\n",
      "Epoch 54/100\n",
      "37560/37560 [==============================] - 18s 485us/step - loss: 0.4973 - acc: 0.7435\n",
      "Epoch 55/100\n",
      "37560/37560 [==============================] - 22s 584us/step - loss: 0.4958 - acc: 0.7456\n",
      "Epoch 56/100\n",
      "37560/37560 [==============================] - 22s 596us/step - loss: 0.4970 - acc: 0.7428\n",
      "Epoch 57/100\n",
      "37560/37560 [==============================] - 22s 592us/step - loss: 0.4977 - acc: 0.7446\n",
      "Epoch 58/100\n",
      "37560/37560 [==============================] - 22s 595us/step - loss: 0.4954 - acc: 0.7461\n",
      "Epoch 59/100\n",
      "37560/37560 [==============================] - 22s 589us/step - loss: 0.4942 - acc: 0.7474\n",
      "Epoch 60/100\n",
      "37560/37560 [==============================] - 21s 570us/step - loss: 0.4972 - acc: 0.7440\n",
      "Epoch 61/100\n",
      "37560/37560 [==============================] - 19s 513us/step - loss: 0.4945 - acc: 0.7449\n",
      "Epoch 62/100\n",
      "37560/37560 [==============================] - 13s 356us/step - loss: 0.4942 - acc: 0.7433\n",
      "Epoch 63/100\n",
      "37560/37560 [==============================] - 13s 350us/step - loss: 0.4918 - acc: 0.7480\n",
      "Epoch 64/100\n",
      "37560/37560 [==============================] - 13s 350us/step - loss: 0.4928 - acc: 0.7451\n",
      "Epoch 65/100\n",
      "37560/37560 [==============================] - 13s 349us/step - loss: 0.4936 - acc: 0.7451\n",
      "Epoch 66/100\n",
      "37560/37560 [==============================] - 13s 348us/step - loss: 0.4925 - acc: 0.7469\n",
      "Epoch 67/100\n",
      "37560/37560 [==============================] - 13s 355us/step - loss: 0.4921 - acc: 0.7459\n",
      "Epoch 68/100\n",
      "37560/37560 [==============================] - 16s 427us/step - loss: 0.4910 - acc: 0.7482\n",
      "Epoch 69/100\n",
      "37560/37560 [==============================] - 18s 488us/step - loss: 0.4893 - acc: 0.7469\n",
      "Epoch 70/100\n",
      "37560/37560 [==============================] - 18s 487us/step - loss: 0.4920 - acc: 0.7464\n",
      "Epoch 71/100\n",
      "37560/37560 [==============================] - 19s 494us/step - loss: 0.4905 - acc: 0.7471\n",
      "Epoch 72/100\n",
      "37560/37560 [==============================] - 19s 500us/step - loss: 0.4889 - acc: 0.7471\n",
      "Epoch 73/100\n",
      "37560/37560 [==============================] - 19s 494us/step - loss: 0.4875 - acc: 0.7499\n",
      "Epoch 74/100\n",
      "37560/37560 [==============================] - 19s 493us/step - loss: 0.4895 - acc: 0.7464\n",
      "Epoch 75/100\n",
      "37560/37560 [==============================] - 19s 505us/step - loss: 0.4878 - acc: 0.7477\n",
      "Epoch 76/100\n",
      "37560/37560 [==============================] - 19s 505us/step - loss: 0.4880 - acc: 0.7490\n",
      "Epoch 77/100\n",
      "37560/37560 [==============================] - 19s 495us/step - loss: 0.4881 - acc: 0.7493\n",
      "Epoch 78/100\n",
      "37560/37560 [==============================] - 19s 498us/step - loss: 0.4879 - acc: 0.7514\n",
      "Epoch 79/100\n",
      "37560/37560 [==============================] - 19s 498us/step - loss: 0.4874 - acc: 0.7483\n",
      "Epoch 80/100\n",
      "37560/37560 [==============================] - 19s 495us/step - loss: 0.4871 - acc: 0.7515\n",
      "Epoch 81/100\n",
      "37560/37560 [==============================] - 19s 496us/step - loss: 0.4882 - acc: 0.7495\n",
      "Epoch 82/100\n",
      "37560/37560 [==============================] - 19s 499us/step - loss: 0.4824 - acc: 0.7521\n",
      "Epoch 83/100\n",
      "37560/37560 [==============================] - 19s 495us/step - loss: 0.4870 - acc: 0.7494\n",
      "Epoch 84/100\n",
      "37560/37560 [==============================] - 19s 494us/step - loss: 0.4846 - acc: 0.7503\n",
      "Epoch 85/100\n",
      "37560/37560 [==============================] - 16s 430us/step - loss: 0.4854 - acc: 0.7522\n",
      "Epoch 86/100\n",
      "37560/37560 [==============================] - 17s 460us/step - loss: 0.4841 - acc: 0.7508\n",
      "Epoch 87/100\n",
      "37560/37560 [==============================] - 19s 495us/step - loss: 0.4846 - acc: 0.7522\n",
      "Epoch 88/100\n",
      "37560/37560 [==============================] - 15s 404us/step - loss: 0.4868 - acc: 0.7512\n",
      "Epoch 89/100\n",
      "37560/37560 [==============================] - 15s 387us/step - loss: 0.4835 - acc: 0.7494\n",
      "Epoch 90/100\n",
      "37560/37560 [==============================] - 14s 366us/step - loss: 0.4845 - acc: 0.7505\n",
      "Epoch 91/100\n",
      "37560/37560 [==============================] - 13s 358us/step - loss: 0.4852 - acc: 0.7509\n",
      "Epoch 92/100\n",
      "37560/37560 [==============================] - 13s 358us/step - loss: 0.4829 - acc: 0.7518\n",
      "Epoch 93/100\n",
      "37560/37560 [==============================] - 14s 366us/step - loss: 0.4810 - acc: 0.7530\n",
      "Epoch 94/100\n",
      "37560/37560 [==============================] - 18s 483us/step - loss: 0.4800 - acc: 0.7531\n",
      "Epoch 95/100\n",
      "37560/37560 [==============================] - 16s 414us/step - loss: 0.4791 - acc: 0.7527\n",
      "Epoch 96/100\n",
      "37560/37560 [==============================] - 14s 378us/step - loss: 0.4786 - acc: 0.7534\n",
      "Epoch 97/100\n",
      "37560/37560 [==============================] - 16s 423us/step - loss: 0.4817 - acc: 0.7508\n",
      "Epoch 98/100\n",
      "37560/37560 [==============================] - 18s 476us/step - loss: 0.4793 - acc: 0.7532\n",
      "Epoch 99/100\n",
      "37560/37560 [==============================] - 18s 475us/step - loss: 0.4796 - acc: 0.7545\n",
      "Epoch 100/100\n",
      "37560/37560 [==============================] - 18s 476us/step - loss: 0.4787 - acc: 0.7574\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fea7b825610>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(aX_train, ay_train, epochs=100, batch_size=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37560/37560 [==============================] - 2s 52us/step\n",
      "\n",
      "acc: 77.17%:\n"
     ]
    }
   ],
   "source": [
    "#evaluate model\n",
    "scores = model.evaluate(aX_train, ay_train)\n",
    "print(\"\\n%s: %.2f%%:\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:\t(4041, 274)\n",
      "After standardization (4041, 274)\n",
      "After converting categoricals:\t(4041, 274)\n"
     ]
    }
   ],
   "source": [
    "aX_test = pre_process_data(aX_test, enforce_cols=aX_train.columns)\n",
    "#bX_test = pre_process_data(bX_test, enforce_cols=bX_train.columns)\n",
    "#cX_test = pre_process_data(cX_test, enforce_cols=cX_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "aX_preds= model.predict(aX_test)\n",
    "#round predictions\n",
    "rounded = [round(x[0]) for x in aX_preds]\n",
    "print(rounded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l2_lambda = 0.01 # use 0.0001 as a L2-regularisation factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(200, kernel_initializer = 'uniform', input_dim = b_num_features, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "model.add(Dense(4, kernel_initializer = 'uniform', activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "model.add(Dense(1, kernel_initializer = 'uniform',activation='sigmoid'))\n",
    "adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "# kernel_regularizer=l2(l2_lambda),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#sgd = SGD(lr=0.01, momentum=0.8, decay=0.0, nesterov=False)\n",
    "#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "20252/20252 [==============================] - 5s 224us/step - loss: 0.4987 - acc: 0.9065\n",
      "Epoch 2/30\n",
      "20252/20252 [==============================] - 4s 215us/step - loss: 0.4249 - acc: 0.9073\n",
      "Epoch 3/30\n",
      "20252/20252 [==============================] - 4s 217us/step - loss: 0.3813 - acc: 0.9073\n",
      "Epoch 4/30\n",
      "20252/20252 [==============================] - 4s 218us/step - loss: 0.3516 - acc: 0.9073\n",
      "Epoch 5/30\n",
      "20252/20252 [==============================] - 4s 217us/step - loss: 0.3291 - acc: 0.9073\n",
      "Epoch 6/30\n",
      "20252/20252 [==============================] - 4s 216us/step - loss: 0.3178 - acc: 0.9073\n",
      "Epoch 7/30\n",
      "20252/20252 [==============================] - 4s 217us/step - loss: 0.3108 - acc: 0.9073\n",
      "Epoch 8/30\n",
      "20252/20252 [==============================] - 4s 218us/step - loss: 0.3060 - acc: 0.9073\n",
      "Epoch 9/30\n",
      "20252/20252 [==============================] - 4s 220us/step - loss: 0.3035 - acc: 0.9073\n",
      "Epoch 10/30\n",
      "20252/20252 [==============================] - 4s 222us/step - loss: 0.3024 - acc: 0.9073\n",
      "Epoch 11/30\n",
      "20252/20252 [==============================] - 4s 220us/step - loss: 0.3006 - acc: 0.9073\n",
      "Epoch 12/30\n",
      "20252/20252 [==============================] - 4s 219us/step - loss: 0.3002 - acc: 0.9073\n",
      "Epoch 13/30\n",
      "20252/20252 [==============================] - 4s 222us/step - loss: 0.3009 - acc: 0.9073\n",
      "Epoch 14/30\n",
      "20252/20252 [==============================] - 5s 231us/step - loss: 0.2992 - acc: 0.9073\n",
      "Epoch 15/30\n",
      "20252/20252 [==============================] - 5s 233us/step - loss: 0.3001 - acc: 0.9073\n",
      "Epoch 16/30\n",
      "20252/20252 [==============================] - 5s 249us/step - loss: 0.2995 - acc: 0.9073\n",
      "Epoch 17/30\n",
      "20252/20252 [==============================] - 5s 239us/step - loss: 0.2981 - acc: 0.9073\n",
      "Epoch 18/30\n",
      "20252/20252 [==============================] - 5s 238us/step - loss: 0.2993 - acc: 0.9073\n",
      "Epoch 19/30\n",
      "20252/20252 [==============================] - 5s 235us/step - loss: 0.2981 - acc: 0.9073\n",
      "Epoch 20/30\n",
      "20252/20252 [==============================] - 5s 236us/step - loss: 0.2992 - acc: 0.9073\n",
      "Epoch 21/30\n",
      "20252/20252 [==============================] - 5s 238us/step - loss: 0.2979 - acc: 0.9073\n",
      "Epoch 22/30\n",
      "20252/20252 [==============================] - 5s 236us/step - loss: 0.2985 - acc: 0.9073\n",
      "Epoch 23/30\n",
      "20252/20252 [==============================] - 5s 237us/step - loss: 0.2986 - acc: 0.9073\n",
      "Epoch 24/30\n",
      "20252/20252 [==============================] - 5s 237us/step - loss: 0.2968 - acc: 0.9073\n",
      "Epoch 25/30\n",
      "20252/20252 [==============================] - 5s 240us/step - loss: 0.2991 - acc: 0.9073\n",
      "Epoch 26/30\n",
      "20252/20252 [==============================] - 5s 238us/step - loss: 0.2997 - acc: 0.9073\n",
      "Epoch 27/30\n",
      "20252/20252 [==============================] - 5s 239us/step - loss: 0.2975 - acc: 0.9073\n",
      "Epoch 28/30\n",
      "20252/20252 [==============================] - 5s 236us/step - loss: 0.2974 - acc: 0.9073\n",
      "Epoch 29/30\n",
      "20252/20252 [==============================] - 5s 244us/step - loss: 0.2955 - acc: 0.9073\n",
      "Epoch 30/30\n",
      "20252/20252 [==============================] - 5s 244us/step - loss: 0.2959 - acc: 0.9073\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2ff4895690>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(bX_train, by_train, epochs=30, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20252/20252 [==============================] - 2s 86us/step\n",
      "\n",
      "acc: 90.73%:\n"
     ]
    }
   ],
   "source": [
    "#evaluate model\n",
    "scores = model.evaluate(bX_train, by_train)\n",
    "print(\"\\n%s: %.2f%%:\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:\t(1604, 441)\n",
      "After standardization (1604, 441)\n",
      "After converting categoricals:\t(1604, 1419)\n"
     ]
    }
   ],
   "source": [
    "bX_test = pre_process_data(b_test, enforce_cols=bX_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "bX_preds = model.predict(bX_test)\n",
    "#round predictions\n",
    "rounded = [round(x[0]) for x in bX_preds]\n",
    "print(rounded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#l3_lambda = 0.001 # use 0.01 as a L2-regularisation factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(200, kernel_initializer = 'uniform', input_dim = c_num_features, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "model.add(Dense(4, kernel_initializer = 'uniform', activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "model.add(Dense(1, kernel_initializer = 'uniform', activation='sigmoid'))\n",
    "adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#sgd = SGD(lr=0.01, momentum=0.8, decay=0.0, nesterov=False)\n",
    "#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "29913/29913 [==============================] - 3s 95us/step - loss: 0.5870 - acc: 0.7644\n",
      "Epoch 2/30\n",
      "29913/29913 [==============================] - 3s 87us/step - loss: 0.5391 - acc: 0.7643\n",
      "Epoch 3/30\n",
      "29913/29913 [==============================] - 3s 87us/step - loss: 0.5262 - acc: 0.7649\n",
      "Epoch 4/30\n",
      "29913/29913 [==============================] - 3s 88us/step - loss: 0.5216 - acc: 0.7653\n",
      "Epoch 5/30\n",
      "29913/29913 [==============================] - 3s 88us/step - loss: 0.5188 - acc: 0.7655\n",
      "Epoch 6/30\n",
      "29913/29913 [==============================] - 3s 88us/step - loss: 0.5191 - acc: 0.7656\n",
      "Epoch 7/30\n",
      "29913/29913 [==============================] - 3s 88us/step - loss: 0.5176 - acc: 0.7661\n",
      "Epoch 8/30\n",
      "29913/29913 [==============================] - 3s 89us/step - loss: 0.5172 - acc: 0.7671\n",
      "Epoch 9/30\n",
      "29913/29913 [==============================] - 3s 88us/step - loss: 0.5174 - acc: 0.7655\n",
      "Epoch 10/30\n",
      "29913/29913 [==============================] - 3s 88us/step - loss: 0.5194 - acc: 0.7661\n",
      "Epoch 11/30\n",
      "29913/29913 [==============================] - 3s 89us/step - loss: 0.5157 - acc: 0.7672\n",
      "Epoch 12/30\n",
      "29913/29913 [==============================] - 3s 90us/step - loss: 0.5171 - acc: 0.7666\n",
      "Epoch 13/30\n",
      "29913/29913 [==============================] - 3s 89us/step - loss: 0.5145 - acc: 0.7672\n",
      "Epoch 14/30\n",
      "29913/29913 [==============================] - 3s 90us/step - loss: 0.5161 - acc: 0.7669\n",
      "Epoch 15/30\n",
      "29913/29913 [==============================] - 3s 90us/step - loss: 0.5169 - acc: 0.7657\n",
      "Epoch 16/30\n",
      "29913/29913 [==============================] - 3s 89us/step - loss: 0.5159 - acc: 0.7683\n",
      "Epoch 17/30\n",
      "29913/29913 [==============================] - 3s 90us/step - loss: 0.5166 - acc: 0.7662\n",
      "Epoch 18/30\n",
      "29913/29913 [==============================] - 3s 90us/step - loss: 0.5164 - acc: 0.7667\n",
      "Epoch 19/30\n",
      "29913/29913 [==============================] - 3s 91us/step - loss: 0.5154 - acc: 0.7668\n",
      "Epoch 20/30\n",
      "29913/29913 [==============================] - 3s 89us/step - loss: 0.5143 - acc: 0.7684\n",
      "Epoch 21/30\n",
      "29913/29913 [==============================] - 3s 91us/step - loss: 0.5145 - acc: 0.7661\n",
      "Epoch 22/30\n",
      "29913/29913 [==============================] - 3s 91us/step - loss: 0.5142 - acc: 0.7671\n",
      "Epoch 23/30\n",
      "29913/29913 [==============================] - 3s 95us/step - loss: 0.5147 - acc: 0.7660\n",
      "Epoch 24/30\n",
      "29913/29913 [==============================] - 3s 91us/step - loss: 0.5153 - acc: 0.7666\n",
      "Epoch 25/30\n",
      "29913/29913 [==============================] - 3s 92us/step - loss: 0.5136 - acc: 0.7686\n",
      "Epoch 26/30\n",
      "29913/29913 [==============================] - 3s 92us/step - loss: 0.5150 - acc: 0.7667\n",
      "Epoch 27/30\n",
      "29913/29913 [==============================] - 3s 91us/step - loss: 0.5140 - acc: 0.7669\n",
      "Epoch 28/30\n",
      "29913/29913 [==============================] - 3s 93us/step - loss: 0.5139 - acc: 0.7664\n",
      "Epoch 29/30\n",
      "29913/29913 [==============================] - 3s 92us/step - loss: 0.5138 - acc: 0.7670\n",
      "Epoch 30/30\n",
      "29913/29913 [==============================] - 3s 92us/step - loss: 0.5135 - acc: 0.7673\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f301001ed50>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(cX_train, cy_train, epochs=30, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29913/29913 [==============================] - 2s 50us/step\n",
      "\n",
      "acc: 76.45%:\n"
     ]
    }
   ],
   "source": [
    "#evaluate model\n",
    "scores = model.evaluate(cX_train, cy_train)\n",
    "print(\"\\n%s: %.2f%%:\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:\t(3187, 163)\n",
      "After standardization (3187, 163)\n",
      "After converting categoricals:\t(3187, 773)\n"
     ]
    }
   ],
   "source": [
    "cX_test = pre_process_data(c_test, enforce_cols=cX_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "cX_preds = model.predict(cX_test)\n",
    "#round predictions\n",
    "rounded = [round(x[0]) for x in cX_preds]\n",
    "print(rounded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_country_sub(preds, test_feat, country):\n",
    "    # make sure we code the country correctly\n",
    "    country_codes = ['A', 'B', 'C']\n",
    "    \n",
    "    # get just the poor probabilities\n",
    "    country_sub = pd.DataFrame(data=preds[:, 0],  # proba p=1\n",
    "                               columns=['poor'], \n",
    "                               index=test_feat.index)\n",
    "\n",
    "    \n",
    "    # add the country code for joining later\n",
    "    country_sub[\"country\"] = country\n",
    "    return country_sub[[\"country\", \"poor\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bX_preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-6c53cf7d38f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# convert preds to data frames\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0ma_sub\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_country_sub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maX_preds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'A'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mb_sub\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_country_sub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbX_preds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'B'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mc_sub\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_country_sub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcX_preds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'C'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'bX_preds' is not defined"
     ]
    }
   ],
   "source": [
    "# convert preds to data frames\n",
    "a_sub = make_country_sub(aX_preds, aX_test, 'A')\n",
    "b_sub = make_country_sub(bX_preds, bX_test, 'B')\n",
    "c_sub = make_country_sub(cX_preds, cX_test, 'C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'b_sub' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-b1bf6d561343>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msubmission\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma_sub\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_sub\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_sub\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'b_sub' is not defined"
     ]
    }
   ],
   "source": [
    "submission = pd.concat([a_sub, b_sub, c_sub])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>poor</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>A</td>\n",
       "      <td>0.526383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41249</th>\n",
       "      <td>A</td>\n",
       "      <td>0.526383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16205</th>\n",
       "      <td>A</td>\n",
       "      <td>0.526383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97501</th>\n",
       "      <td>A</td>\n",
       "      <td>0.526383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67756</th>\n",
       "      <td>A</td>\n",
       "      <td>0.526383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17938</th>\n",
       "      <td>A</td>\n",
       "      <td>0.526383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19036</th>\n",
       "      <td>A</td>\n",
       "      <td>0.526383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61587</th>\n",
       "      <td>A</td>\n",
       "      <td>0.526383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57571</th>\n",
       "      <td>A</td>\n",
       "      <td>0.526383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64203</th>\n",
       "      <td>A</td>\n",
       "      <td>0.526383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30762</th>\n",
       "      <td>A</td>\n",
       "      <td>0.526383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41072</th>\n",
       "      <td>A</td>\n",
       "      <td>0.526383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46727</th>\n",
       "      <td>A</td>\n",
       "      <td>0.526383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78263</th>\n",
       "      <td>A</td>\n",
       "      <td>0.526383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58110</th>\n",
       "      <td>A</td>\n",
       "      <td>0.526383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13124</th>\n",
       "      <td>A</td>\n",
       "      <td>0.526383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71752</th>\n",
       "      <td>A</td>\n",
       "      <td>0.526383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75036</th>\n",
       "      <td>A</td>\n",
       "      <td>0.526383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68309</th>\n",
       "      <td>A</td>\n",
       "      <td>0.526383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5668</th>\n",
       "      <td>A</td>\n",
       "      <td>0.526383</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      country      poor\n",
       "id                     \n",
       "418         A  0.526383\n",
       "41249       A  0.526383\n",
       "16205       A  0.526383\n",
       "97501       A  0.526383\n",
       "67756       A  0.526383\n",
       "17938       A  0.526383\n",
       "19036       A  0.526383\n",
       "61587       A  0.526383\n",
       "57571       A  0.526383\n",
       "64203       A  0.526383\n",
       "30762       A  0.526383\n",
       "41072       A  0.526383\n",
       "46727       A  0.526383\n",
       "78263       A  0.526383\n",
       "58110       A  0.526383\n",
       "13124       A  0.526383\n",
       "71752       A  0.526383\n",
       "75036       A  0.526383\n",
       "68309       A  0.526383\n",
       "5668        A  0.526383"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>poor</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>88475</th>\n",
       "      <td>C</td>\n",
       "      <td>0.297106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96504</th>\n",
       "      <td>C</td>\n",
       "      <td>0.297106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51200</th>\n",
       "      <td>C</td>\n",
       "      <td>0.297106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94912</th>\n",
       "      <td>C</td>\n",
       "      <td>0.297106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7588</th>\n",
       "      <td>C</td>\n",
       "      <td>0.297106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35668</th>\n",
       "      <td>C</td>\n",
       "      <td>0.297106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6137</th>\n",
       "      <td>C</td>\n",
       "      <td>0.297106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87474</th>\n",
       "      <td>C</td>\n",
       "      <td>0.297106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99992</th>\n",
       "      <td>C</td>\n",
       "      <td>0.297106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8544</th>\n",
       "      <td>C</td>\n",
       "      <td>0.297106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63899</th>\n",
       "      <td>C</td>\n",
       "      <td>0.297106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72790</th>\n",
       "      <td>C</td>\n",
       "      <td>0.297106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1588</th>\n",
       "      <td>C</td>\n",
       "      <td>0.297106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>C</td>\n",
       "      <td>0.297106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68171</th>\n",
       "      <td>C</td>\n",
       "      <td>0.297106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6775</th>\n",
       "      <td>C</td>\n",
       "      <td>0.297106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88300</th>\n",
       "      <td>C</td>\n",
       "      <td>0.297106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35424</th>\n",
       "      <td>C</td>\n",
       "      <td>0.297106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81668</th>\n",
       "      <td>C</td>\n",
       "      <td>0.297106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98377</th>\n",
       "      <td>C</td>\n",
       "      <td>0.297106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      country      poor\n",
       "id                     \n",
       "88475       C  0.297106\n",
       "96504       C  0.297106\n",
       "51200       C  0.297106\n",
       "94912       C  0.297106\n",
       "7588        C  0.297106\n",
       "35668       C  0.297106\n",
       "6137        C  0.297106\n",
       "87474       C  0.297106\n",
       "99992       C  0.297106\n",
       "8544        C  0.297106\n",
       "63899       C  0.297106\n",
       "72790       C  0.297106\n",
       "1588        C  0.297106\n",
       "254         C  0.297106\n",
       "68171       C  0.297106\n",
       "6775        C  0.297106\n",
       "88300       C  0.297106\n",
       "35424       C  0.297106\n",
       "81668       C  0.297106\n",
       "98377       C  0.297106"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
